name: Bootstrap Project

on:
  workflow_dispatch:

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Create project files
        run: |
          mkdir -p tasks data/input_pdfs data/raw data/processed maps templates .github/workflows

          cat > requirements.txt << 'EOF'
          flask==3.0.3
          pdfplumber==0.11.4
          pandas==2.2.2
          folium==0.17.0
          python-dotenv==1.0.1
          requests==2.32.3
          tqdm==4.66.4
          gunicorn==22.0.0
          beautifulsoup4==4.12.3
          EOF

          cat > .env.example << 'EOF'
          OPENCAGE_API_KEY=
          MAPBOX_ACCESS_TOKEN=
          GEOCODER_PREF=opencage
          FLASK_RUN_PORT=8000
          FLASK_DEBUG=0
          EOF

          cat > tasks/discover_pdfs.py << 'EOF'
          import re, json
          from pathlib import Path
          from bs4 import BeautifulSoup
          import requests
          BASE = "https://floridalottery.com"
          HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; ScratchMapWeb/1.1)"}
          def _get(url):
              r = requests.get(url, headers=HEADERS, timeout=30); r.raise_for_status(); return r.text
          def discover_all_games():
              games = {}
              try:
                  html = _get(f"{BASE}/games/scratch-offs/top-remaining-prizes")
                  soup = BeautifulSoup(html, "html.parser")
                  for a in soup.select("a[href*='games/scratch-offs/view?id=']"):
                      href = a.get("href"); gid = re.search(r"id=(\\d+)", href)
                      if gid: games[gid.group(1)] = {"game_id": gid.group(1), "game_name": a.get_text(strip=True)}
              except Exception: pass
              try:
                  html = _get(f"{BASE}/games/scratch-offs")
                  soup = BeautifulSoup(html, "html.parser")
                  for a in soup.select("a[href*='games/scratch-offs/view?id=']"):
                      href = a.get("href"); gid = re.search(r"id=(\\d+)", href)
                      if gid: games.setdefault(gid.group(1), {"game_id": gid.group(1), "game_name": a.get_text(strip=True)})
              except Exception: pass
              results = []
              for gid, meta in games.items():
                  try:
                      page_html = _get(f"{BASE}/games/scratch-offs/view?id={gid}")
                      s = BeautifulSoup(page_html, "html.parser"); pdf_url = None
                      for a in s.find_all("a", href=True):
                          href = a['href']; text = a.get_text(" ", strip=True).lower()
                          if "winning ticket information" in text or href.endswith("WinningTicketInformation.pdf") or "WinningTicketInformation" in href:
                              if href.startswith("http"): pdf_url = href
                              elif href.startswith("//"): pdf_url = "https:" + href
                              else: pdf_url = BASE + href if href.startswith("/") else f"{BASE}/{href}"
                              break
                      if pdf_url:
                          name = meta.get("game_name") or gid
                          results.append({"game_id": gid, "game_name": name, "pdf_url": pdf_url})
                  except Exception: continue
              seen=set(); ded=[]
              for g in results:
                  key=(g["game_id"], g["pdf_url"])
                  if key not in seen: seen.add(key); ded.append(g)
              return ded
          def write_games_json(path="games.json"):
              games = discover_all_games()
              with open(path, "w", encoding="utf-8") as f:
                  json.dump({"games": games}, f, indent=2)
              return games
          if __name__ == "__main__":
              out = write_games_json(); print(f"Discovered {len(out)} games with PDFs")
          EOF

          cat > tasks/scrape.py << 'EOF'
          import json
          from pathlib import Path
          import requests
          from tqdm import tqdm
          HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; ScratchMapWeb/1.0)"}
          def fetch_pdfs(config_path, out_dir: Path):
              out_dir.mkdir(parents=True, exist_ok=True)
              cfg = json.loads(Path(config_path).read_text())
              for game in cfg.get("games", []):
                  url = game["pdf_url"]; game_id = game["game_id"]; fname = out_dir / f"{game_id}.pdf"
                  print(f"Downloading {game_id} from {url} ...")
                  try:
                      with requests.get(url, headers=HEADERS, stream=True, timeout=30) as r:
                          r.raise_for_status()
                          total = int(r.headers.get("content-length", 0))
                          with open(fname, "wb") as f, tqdm(total=total, unit="B", unit_scale=True) as bar:
                              for chunk in r.iter_content(chunk_size=8192):
                                  if chunk: f.write(chunk); bar.update(len(chunk))
                  except Exception as e:
                      print(f"  ⚠️ Failed to fetch {url}: {e}")
          EOF

          cat > tasks/parse_pdf.py << 'EOF'
          from pathlib import Path
          import re, pdfplumber, pandas as pd, sqlite3
          CITY_STATE_ZIP_RE = re.compile(r"(.+?),\\s*FL\\s+(\\d{5})(?:-\\d{4})?$", re.IGNORECASE)
          def parse_pdf(pdf_path: Path, game_id: str, game_name: str):
              rows=[]
              try:
                  with pdfplumber.open(pdf_path) as pdf:
                      for page in pdf.pages:
                          text = page.extract_text() or ""
                          for line in text.splitlines():
                              if ", FL " in line.upper() and "$" in line:
                                  parts=[p.strip() for p in line.replace("—","-").split("-")]
                                  prize = next((p for p in parts if "$" in p), None)
                                  date=None
                                  for p in parts[::-1]:
                                      if re.search(r"\\d{4}-\\d{2}-\\d{2}", p) or re.search(r"\\d{1,2}/\\d{1,2}/\\d{2,4}", p):
                                          date=p; break
                                  retailer_chunk = line.split("$")[0].strip(" -")
                                  try:
                                      retailer, address = retailer_chunk.rsplit(",",1); retailer=retailer.strip(); address=address.strip()
                                  except ValueError:
                                      retailer=retailer_chunk; address=""
                                  m = CITY_STATE_ZIP_RE.search(line); city = zip_code = None
                                  if m:
                                      city = m.group(1).split(",")[-1].strip()
                                      zip_code = m.group(2)
                                  rows.append({"game_id":game_id,"game_name":game_name,"retailer":retailer,"address_full":retailer_chunk,"street":address,"city":city,"state":"FL","zip":zip_code,"prize":prize,"claim_date":date})
              except Exception as e:
                  print(f"⚠️ PDF parse failed for {pdf_path.name}: {e}")
              return rows
          def parse_all_pdfs(input_pdf_dir: Path, raw_dir: Path, processed_dir: Path, db_path: Path):
              processed_dir.mkdir(parents=True, exist_ok=True); all_rows=[]
              override_csv = raw_dir / "input_override.csv"
              if override_csv.exists(): df_override = pd.read_csv(override_csv)
              else: df_override = pd.DataFrame(columns=["game_id","game_name","retailer","street","city","state","zip","prize","claim_date"])
              for pdf in input_pdf_dir.glob("*.pdf"):
                  game_id = pdf.stem; game_name = game_id
                  rows = parse_pdf(pdf, game_id, game_name); all_rows.extend(rows)
              df_pdf = pd.DataFrame(all_rows)
              df = pd.concat([df_pdf, df_override], ignore_index=True, axis=0)
              df.drop_duplicates(subset=["game_id","retailer","address_full","prize"], inplace=True)
              if "prize" in df.columns:
                  df["prize_amount"] = pd.to_numeric(df["prize"].str.replace(r"[^0-9.]", "", regex=True), errors="coerce")
              out_csv = processed_dir / "winners.csv"; df.to_csv(out_csv, index=False)
              conn = sqlite3.connect(db_path); df.to_sql("winners", conn, if_exists="replace", index=False); conn.close()
              return df
          EOF

          cat > tasks/geocode.py << 'EOF'
          import os, time, requests, pandas as pd, sqlite3
          def _geocode_opencage(addr, key):
              url="https://api.opencagedata.com/geocode/v1/json"; r=requests.get(url, params={"q":addr,"key":key,"limit":1}, timeout=20); r.raise_for_status()
              js=r.json(); 
              if js.get("results"): g=js["results"][0]["geometry"]; return g.get("lat"), g.get("lng")
              return None,None
          def _geocode_mapbox(addr, token):
              import requests as rq
              url=f"https://api.mapbox.com/geocoding/v5/mapbox.places/{rq.utils.quote(addr)}.json"
              r=rq.get(url, params={"access_token":token,"limit":1}, timeout=20); r.raise_for_status(); js=r.json()
              if js.get("features"): coords=js["features"][0]["geometry"]["coordinates"]; return coords[1], coords[0]
              return None,None
          def _geocode_nominatim(addr):
              url="https://nominatim.openstreetmap.org/search"
              r=requests.get(url, params={"q":addr,"format":"json","limit":1}, headers={"User-Agent":"ScratchMapWeb/1.0"}, timeout=20); r.raise_for_status()
              js=r.json(); 
              if js: return float(js[0]["lat"]), float(js[0]["lon"])
              return None,None
          def geocode_missing(df, db_path):
              if "lat" not in df.columns: df["lat"]=None
              if "lon" not in df.columns: df["lon"]=None
              pref=os.getenv("GEOCODER_PREF","nominatim").lower()
              oc_key=os.getenv("OPENCAGE_API_KEY","").strip(); mb_key=os.getenv("MAPBOX_ACCESS_TOKEN","").strip()
              def geocode(addr):
                  if not addr: return (None,None)
                  if pref=="opencage" and oc_key:
                      try: return _geocode_opencage(addr, oc_key)
                      except Exception: pass
                  if pref=="mapbox" and mb_key:
                      try: return _geocode_mapbox(addr, mb_key)
                      except Exception: pass
                  try: time.sleep(1.1); return _geocode_nominatim(addr)
                  except Exception: return (None,None)
              def mk_addr(row):
                  parts=[]; 
                  for k in ["street","city","state","zip"]:
                      v=row.get(k)
                      if pd.notna(v) and str(v).strip(): parts.append(str(v).strip())
                  addr=", ".join(parts)
                  if not addr and row.get("address_full"): addr=str(row["address_full"])
                  return addr
              df["addr_query"]=df.apply(mk_addr, axis=1)
              for idx,row in df[df["lat"].isna()].iterrows():
                  lat,lon=geocode(row["addr_query"]); df.at[idx,"lat"]=lat; df.at[idx,"lon"]=lon
              conn=sqlite3.connect(db_path); df.to_sql("winners", conn, if_exists="replace", index=False); conn.close(); return df
          EOF

          cat > tasks/build_map.py << 'EOF'
          import folium
          from folium.plugins import MarkerCluster
          from pathlib import Path
          def _format_popup(row):
              prize=row.get("prize") or ""; date=row.get("claim_date") or ""
              game=row.get("game_name") or row.get("game_id") or ""
              retailer=row.get("retailer") or ""; addr=row.get("addr_query") or ""
              return (f"<b>{retailer}</b><br/>{addr}<br/><b>Game:</b> {game}<br/><b>Prize:</b> {prize}<br/><b>Claimed:</b> {date}")
          def build_map(df, out_html: Path):
              df_map=df.dropna(subset=["lat","lon"]).copy()
              if df_map.empty:
                  m=folium.Map(location=[27.6648,-81.5158], zoom_start=6); m.save(out_html); return
              center_lat=df_map["lat"].median(); center_lon=df_map["lon"].median()
              m=folium.Map(location=[center_lat,center_lon], zoom_start=6); cluster=MarkerCluster().add_to(m)
              def color_for_prize(val):
                  try: amount=float(str(val).replace(",","").replace("$",""))
                  except: amount=0.0
                  if amount>=1_000_000: return "red"
                  if amount>=100_000: return "orange"
                  if amount>=10_000: return "green"
                  return "blue"
              for _,row in df_map.iterrows():
                  popup_html=_format_popup(row)
                  folium.CircleMarker(location=[row["lat"],row["lon"]], radius=6,
                                      color=color_for_prize(row.get("prize")),
                                      fill=True, fill_opacity=0.8,
                                      popup=folium.Popup(popup_html, max_width=300)).add_to(cluster)
              m.save(out_html)
          EOF

          cat > app.py << 'EOF'
          import os
          from pathlib import Path
          from flask import Flask, render_template, send_from_directory, jsonify
          from dotenv import load_dotenv
          from tasks.discover_pdfs import write_games_json
          from tasks.scrape import fetch_pdfs
          from tasks.parse_pdf import parse_all_pdfs
          from tasks.geocode import geocode_missing
          from tasks.build_map import build_map
          load_dotenv()
          app=Flask(__name__)
          DATA_DIR=Path("data"); MAPS_DIR=Path("maps"); RAW_DIR=DATA_DIR/"raw"; INPUT_PDF_DIR=Path("data/input_pdfs"); PROCESSED_DIR=DATA_DIR/"processed"; DB_PATH=PROCESSED_DIR/"winners.sqlite"
          for p in [MAPS_DIR,RAW_DIR,INPUT_PDF_DIR,PROCESSED_DIR]: p.mkdir(parents=True, exist_ok=True)
          @app.route("/")
          def index():
              return "Use GitHub Pages workflow for public site. /map serves the local HTML if running Flask."
          @app.route("/map")
          def map_view():
              if (MAPS_DIR/"fl_winners_map.html").exists(): return send_from_directory(MAPS_DIR, "fl_winners_map.html")
              return "No map yet.", 404
          if __name__=="__main__":
              app.run(host="0.0.0.0", port=int(os.environ.get("FLASK_RUN_PORT","8000")))
          EOF

          cat > templates/index.html << 'EOF'
          <!doctype html><html><body><p>Static build uses GitHub Pages. This template is only for local run.</p></body></html>
          EOF

          cat > .github/workflows/build_publish.yml << 'EOF'
          name: Build & Publish Map (Nightly)
          on:
            schedule:
              - cron: "17 5 * * *"
            workflow_dispatch:
          jobs:
            build:
              runs-on: ubuntu-latest
              steps:
                - uses: actions/checkout@v4
                - uses: actions/setup-python@v5
                  with: { python-version: '3.11' }
                - run: |
                    python -m pip install --upgrade pip
                    pip install -r requirements.txt
                - name: Build latest map
                  env:
                    OPENCAGE_API_KEY: ${{ secrets.OPENCAGE_API_KEY }}
                    MAPBOX_ACCESS_TOKEN: ${{ secrets.MAPBOX_ACCESS_TOKEN }}
                    GEOCODER_PREF: ${{ secrets.GEOCODER_PREF }}
                  run: |
                    python - << 'PY'
                    from tasks.discover_pdfs import write_games_json
                    from tasks.scrape import fetch_pdfs
                    from tasks.parse_pdf import parse_all_pdfs
                    from tasks.geocode import geocode_missing
                    from tasks.build_map import build_map
                    from pathlib import Path
                    DATA_DIR=Path("data"); MAPS_DIR=Path("maps"); RAW_DIR=DATA_DIR/"raw"; INPUT_PDF_DIR=Path("data/input_pdfs"); PROCESSED_DIR=DATA_DIR/"processed"; DB_PATH=PROCESSED_DIR/"winners.sqlite"
                    for p in [MAPS_DIR,RAW_DIR,INPUT_PDF_DIR,PROCESSED_DIR]: p.mkdir(parents=True, exist_ok=True)
                    write_games_json("games.json")
                    fetch_pdfs("games.json", INPUT_PDF_DIR)
                    df=parse_all_pdfs(INPUT_PDF_DIR, RAW_DIR, PROCESSED_DIR, DB_PATH)
                    df=geocode_missing(df, DB_PATH)
                    build_map(df, out_html=MAPS_DIR/"fl_winners_map.html")
                    PY
                - uses: actions/upload-pages-artifact@v3
                  with: { path: maps }
            deploy:
              needs: build
              permissions: { pages: write, id-token: write }
              runs-on: ubuntu-latest
              environment: { name: github-pages, url: ${{ steps.deployment.outputs.page_url }} }
              steps:
                - id: deployment
                  uses: actions/deploy-pages@v4
          EOF

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "Bootstrap: add scratch-off map app + nightly Pages workflow"
          git push
      - name: Done
        run: echo "Bootstrap complete. Now run the 'Build & Publish Map (Nightly)' workflow."
