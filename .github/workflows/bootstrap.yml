name: Bootstrap Project (Clean)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Create project files and commit
        run: |
          set -e
          mkdir -p tasks data/input_pdfs data/raw data/processed maps templates .github/workflows

          cat > requirements.txt << 'EOF'
flask==3.0.3
pdfplumber==0.11.4
pandas==2.2.2
folium==0.17.0
python-dotenv==1.0.1
requests==2.32.3
tqdm==4.66.4
gunicorn==22.0.0
beautifulsoup4==4.12.3
EOF

          cat > tasks/discover_pdfs.py << 'EOF'
import re, json
from bs4 import BeautifulSoup
import requests
BASE="https://floridalottery.com"
HEADERS={"User-Agent":"Mozilla/5.0 (ScratchMapWeb/1.1)"}
def _get(u): r=requests.get(u,headers=HEADERS,timeout=30); r.raise_for_status(); return r.text
def discover_all_games():
  games={}
  try:
    s=_get(f"{BASE}/games/scratch-offs/top-remaining-prizes")
    soup=BeautifulSoup(s,"html.parser")
    for a in soup.select("a[href*='games/scratch-offs/view?id=']"):
      h=a.get("href"); gid=re.search(r"id=(\\d+)",h)
      if gid: games[gid.group(1)]={"game_id":gid.group(1),"game_name":a.get_text(strip=True)}
  except Exception: pass
  try:
    s=_get(f"{BASE}/games/scratch-offs")
    soup=BeautifulSoup(s,"html.parser")
    for a in soup.select("a[href*='games/scratch-offs/view?id=']"):
      h=a.get("href"); gid=re.search(r"id=(\\d+)",h)
      if gid: games.setdefault(gid.group(1),{"game_id":gid.group(1),"game_name":a.get_text(strip=True)})
  except Exception: pass
  res=[]
  for gid,meta in games.items():
    try:
      s=_get(f"{BASE}/games/scratch-offs/view?id={gid}")
      soup=BeautifulSoup(s,"html.parser"); pdf=None
      for a in soup.find_all("a",href=True):
        h=a["href"]; t=a.get_text(" ",strip=True).lower()
        if "winning ticket information" in t or "WinningTicketInformation" in h or h.endswith("WinningTicketInformation.pdf"):
          pdf = ("https:" + h) if h.startswith("//") else (h if h.startswith("http") else BASE + h if h.startswith("/") else f"{BASE}/{h}")
          break
      if pdf: res.append({"game_id":gid,"game_name":meta.get("game_name") or gid,"pdf_url":pdf})
    except Exception: pass
  # de-dupe
  out=[]; seen=set()
  for g in res:
    k=(g["game_id"],g["pdf_url"])
    if k not in seen: seen.add(k); out.append(g)
  return out
def write_games_json(path="games.json"):
  g=discover_all_games()
  with open(path,"w",encoding="utf-8") as f: json.dump({"games":g},f,indent=2)
  return g
if __name__=="__main__":
  print(len(write_games_json()))
EOF

          cat > tasks/scrape.py << 'EOF'
import json, requests
from pathlib import Path
from tqdm import tqdm
HEADERS={"User-Agent":"Mozilla/5.0 (ScratchMapWeb/1.0)"}
def fetch_pdfs(cfg_path, out_dir: Path):
  out_dir.mkdir(parents=True, exist_ok=True)
  cfg=json.loads(Path(cfg_path).read_text())
  for g in cfg.get("games",[]):
    url=g["pdf_url"]; gid=g["game_id"]; fn=out_dir / f"{gid}.pdf"
    try:
      with requests.get(url, headers=HEADERS, stream=True, timeout=30) as r:
        r.raise_for_status()
        total=int(r.headers.get("content-length",0))
        with open(fn,"wb") as f, tqdm(total=total, unit="B", unit_scale=True) as bar:
          for chunk in r.iter_content(8192):
            if chunk: f.write(chunk); bar.update(len(chunk))
    except Exception as e:
      print("Failed:", url, e)
EOF

          cat > tasks/parse_pdf.py << 'EOF'
from pathlib import Path
import re, pdfplumber, pandas as pd, sqlite3
CITY_STATE_ZIP_RE=re.compile(r"(.+?),\\s*FL\\s+(\\d{5})(?:-\\d{4})?$", re.IGNORECASE)
def parse_pdf(pdf_path: Path, game_id: str, game_name: str):
  rows=[]
  try:
    with pdfplumber.open(pdf_path) as pdf:
      for pg in pdf.pages:
        txt = pg.extract_text() or ""
        for line in txt.splitlines():
          if ", FL " in line.upper() and "$" in line:
            parts=[p.strip() for p in line.replace("â€”","-").split("-")]
            prize=next((p for p in parts if "$" in p), None)
            date=None
            for p in parts[::-1]:
              if re.search(r"\\d{4}-\\d{2}-\\d{2}", p) or re.search(r"\\d{1,2}/\\d{1,2}/\\d{2,4}", p):
                date=p; break
            chunk=line.split("$")[0].strip(" -")
            try:
              retailer, street = chunk.rsplit(",",1); retailer=retailer.strip(); street=street.strip()
            except ValueError:
              retailer=chunk; street=""
            m=CITY_STATE_ZIP_RE.search(line); city=zip_code=None
            if m: city=m.group(1).split(",")[-1].strip(); zip_code=m.group(2)
            rows.append({"game_id":game_id,"game_name":game_name,"retailer":retailer,"address_full":chunk,"street":street,"city":city,"state":"FL","zip":zip_code,"prize":prize,"claim_date":date})
  except Exception as e:
    print("PDF parse failed:", pdf_path.name, e)
  return rows
def parse_all_pdfs(input_pdf_dir: Path, raw_dir: Path, processed_dir: Path, db_path: Path):
  processed_dir.mkdir(parents=True, exist_ok=True)
  all_rows=[]
  over=raw_dir/"input_override.csv"
  if over.exists(): df_over=pd.read_csv(over)
  else: df_over=pd.DataFrame(columns=["game_id","game_name","retailer","street","city","state","zip","prize","claim_date"])
  for pdf in input_pdf_dir.glob("*.pdf"):
    gid=pdf.stem; gname=gid; all_rows+=parse_pdf(pdf, gid, gname)
  df_pdf=pd.DataFrame(all_rows)
  df=pd.concat([df_pdf, df_over], ignore_index=True)
  df.drop_duplicates(subset=["game_id","retailer","address_full","prize"], inplace=True)
  if "prize" in df.columns:
    df["prize_amount"]=pd.to_numeric(df["prize"].str.replace(r"[^0-9.]", "", regex=True), errors="coerce")
  (processed_dir/"winners.csv").parent.mkdir(parents=True, exist_ok=True)
  df.to_csv(processed_dir/"winners.csv", index=False)
  conn=sqlite3.connect(db_path); df.to_sql("winners", conn, if_exists="replace", index=False); conn.close()
  return df
EOF

          cat > tasks/geocode.py << 'EOF'
import time, requests, pandas as pd, sqlite3
def _geo_nom(addr):
  r=requests.get("https://nominatim.openstreetmap.org/search", params={"q":addr,"format":"json","limit":1}, headers={"User-Agent":"ScratchMapWeb/1.0"}, timeout=20)
  r.raise_for_status(); js=r.json()
  if js: return float(js[0]["lat"]), float(js[0]["lon"])
  return None,None
def geocode_missing(df, db_path):
  if "lat" not in df.columns: df["lat"]=None
  if "lon" not in df.columns: df["lon"]=None
  def mk_addr(row):
    parts=[]
    for k in ["street","city","state","zip"]:
      v=row.get(k)
      if pd.notna(v) and str(v).strip(): parts.append(str(v).strip())
    addr=", ".join(parts)
    if not addr and row.get("address_full"): addr=str(row["address_full"])
    return addr
  df["addr_query"]=df.apply(mk_addr, axis=1)
  for idx,row in df[df["lat"].isna()].iterrows():
    time.sleep(1.1)
    lat,lon=_geo_nom(row["addr_query"]); df.at[idx,"lat"]=lat; df.at[idx,"lon"]=lon
  conn=sqlite3.connect(db_path); df.to_sql("winners", conn, if_exists="replace", index=False); conn.close(); return df
EOF

          cat > tasks/build_map.py << 'EOF'
import folium
from folium.plugins import MarkerCluster
from pathlib import Path
def _popup(r):
  p=r.get("prize") or ""; d=r.get("claim_date") or ""; g=r.get("game_name") or r.get("game_id") or ""
  ret=r.get("retailer") or ""; a=r.get("addr_query") or ""
  return f"<b>{ret}</b><br/>{a}<br/><b>Game:</b> {g}<br/><b>Prize:</b> {p}<br/><b>Claimed:</b> {d}"
def build_map(df, out_html: Path):
  df=df.dropna(subset=["lat","lon"]).copy()
  if df.empty:
    m=folium.Map(location=[27.6648,-81.5158], zoom_start=6); m.save(out_html); return
  m=folium.Map(location=[df['lat'].median(), df['lon'].median()], zoom_start=6)
  cluster=MarkerCluster().add_to(m)
  def color(val):
    try: amt=float(str(val).replace(",","").replace("$",""))
    except: amt=0.0
    return "red" if amt>=1_000_000 else "orange" if amt>=100_000 else "green" if amt>=10_000 else "blue"
  for _,r in df.iterrows():
    folium.CircleMarker([r["lat"],r["lon"]], radius=6, color=color(r.get("prize")), fill=True, fill_opacity=0.8,
                        popup=folium.Popup(_popup(r), max_width=300)).add_to(cluster)
  m.save(out_html)
EOF

          cat > app.py << 'EOF'
from pathlib import Path
from tasks.discover_pdfs import write_games_json
from tasks.scrape import fetch_pdfs
from tasks.parse_pdf import parse_all_pdfs
from tasks.geocode import geocode_missing
from tasks.build_map import build_map
DATA=Path("data"); MAPS=Path("maps"); RAW=DATA/"raw"; INP=DATA/"input_pdfs"; PRO=DATA/"processed"; DB=PRO/"winners.sqlite"
for p in [MAPS, RAW, INP, PRO]: p.mkdir(parents=True, exist_ok=True)
print("Discovering games..."); write_games_json("games.json")
print("Downloading PDFs..."); fetch_pdfs("games.json", INP)
print("Parsing..."); df=parse_all_pdfs(INP, RAW, PRO, DB)
print("Geocoding (first run is slow)..."); df=geocode_missing(df, DB)
print("Building map..."); build_map(df, out_html=MAPS/"fl_winners_map.html")
print("OK")
EOF

          cat > .github/workflows/build_publish.yml << 'EOF'
name: Build & Publish Map (Nightly)
on:
  schedule:
    - cron: "17 5 * * *"
  workflow_dispatch:
permissions:
  pages: write
  id-token: write
  contents: read
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Build
        run: python app.py
      - uses: actions/upload-pages-artifact@v3
        with:
          path: maps
  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
EOF

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "Bootstrap: add scratch-off map + nightly Pages"
          git push
